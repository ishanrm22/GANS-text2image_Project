{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"P3hDtc9rMxkR"},"source":["> This assignment as two parts Section A and B. The first part is the implementation of function required in applying the CNN layers and the next section will be around the use of built in function of Tensorflow"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"48fGpBzYOjt1"},"source":["\n","\n","`Before moving ahead as we have reached the stage of applying CNNs. We are going to deal with large number of parameters and hence more computational power. So you will need to connect runtime of collab to GPU: https://www.youtube.com/watch?v=-9CLfrZISRw`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4mSFI4ExalwM"},"source":["### **BOTH THE SECTION CAN BE SOLVED INDEPENDENTLY** BUT SECTION A has **3X** more weightage than SECTION B doesn't have any code to write."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CjXMqW4cMrFg"},"source":["## **SECTION:A**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8eFBZbMLxGM"},"outputs":[],"source":["import numpy as np"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LbWwF7vkLxGP"},"source":["### Q1: Complete the following function corr2d(X, K), which implements the cross correlation operation for matrix X and kernel K, both are two dimensional numpy arrays (height x width). The function should return a 2 dimensional numpy array which is the result of cross correlation operation between X and K. \n","\n","- not giving channels right now : assume channels = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rVZCrIDLxGT"},"outputs":[],"source":["import numpy as np\n","\n","def corr2d(X: np.array, K: np.array) -> np.array:\n","    X_height, X_width = X.shape\n","    \n","    if K.ndim == 1:\n","        K = K.reshape((1, K.shape[0]))\n","        K_height, K_width = 1, K.shape[1]\n","    else:\n","        K_height, K_width = K.shape\n","    \n","    output_height = X_height - K_height + 1\n","    output_width = X_width - K_width + 1\n","    output = np.zeros((output_height, output_width))\n","  \n","    for i in range(output_height):\n","        for j in range(output_width):\n","            output[i, j] = np.sum(X[i:i+K_height, j:j+K_width] * K)\n","    \n","    return output \n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":628,"status":"ok","timestamp":1685979387576,"user":{"displayName":"Sapna Dhirawat","userId":"10829815467237624795"},"user_tz":-330},"id":"FYF60FbvLxGd","outputId":"7c31d2d1-fdf9-4725-8d0c-154671d55640"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[19. 25.]\n"," [37. 43.]]\n"]}],"source":["X = np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]], dtype=np.float32)\n","K = np.array([[0.0, 1.0], [2.0, 3.0]], dtype=np.float32)\n","print(corr2d(X, K)) # example done in class, try to print this and check if you get the right answer"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uLMP6bE3LxGf"},"source":["### now try to make a new function corr2d_multiple_input_channels(X, K) : where each X and K have the same number of channels, both of them are now 3 dimensional numpy arrays, the output should be a 2 dimensional numpy array (output_h, output_w).\n","\n","- hint : Use the above corr2d function and read about np.stack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glmBGALxLxGg"},"outputs":[],"source":["def corr2d_multiple_input_channels(X: np.array, K: np.array) -> np.array: \n","    # write a function for this task\n","    num_input_channels, _, _ = X.shape\n","    output = None\n","    \n","    for i in range(num_input_channels):\n","        if output is None:\n","            output = corr2d(X[i], K[i])\n","        else:\n","            output += corr2d(X[i], K[i])\n","    \n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":726,"status":"ok","timestamp":1685979395069,"user":{"displayName":"Sapna Dhirawat","userId":"10829815467237624795"},"user_tz":-330},"id":"Iw3vxttBLxGg","outputId":"2890d3d1-00e2-4248-95b3-363c6fe58e0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["new_X.shape = (3, 3, 3), new_K.shape = (3, 2, 2)\n","[[119. 149.]\n"," [209. 239.]]\n"]}],"source":["new_X = np.stack([X, X+1, X+2], axis=0) # stacking along a new dimension\n","new_K = np.stack([K, K+1, K+2], axis=0) \n","\n","print(f\"new_X.shape = {new_X.shape}, new_K.shape = {new_K.shape}\")\n","print(corr2d_multiple_input_channels(new_X, new_K))\n","# calculate the output by hand and then check whether you get the same answer\n","# answer should be a 2 dim np array : (output_height, output_width) "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fhXuwIj9LxGh"},"source":["### Write another function corr2d_mutli_in_out(X, K): where X (3 dim np array), K (4 dim numpy array), 0th dimension of K represents the number of kernel/filters we are using. Perform the cross correlation operation for K on X and return the output : 3 dim numpy array whose shape should be (num_output_channels, output_height, output_width)\n","\n","- hint : use the above corr_2d_mutliple_input_channels(X, K) for each kernel in K and then stack them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3H-oe3xLxGi"},"outputs":[],"source":["def corr2d_multi_in_out(X: np.array, K: np.array) -> np.array:\n","    # X -> (num_in_channels, n_h, n_w)\n","    # K -> (num_out_channels, num_in_channels, k_h, k_w)\n","    # output -> (num_out_channels, o_h, o_w)\n","    num_out_channels, num_in_channels, _, _ = K.shape\n","    output = None\n","    \n","    for i in range(num_out_channels):\n","        if output is None:\n","            output = corr2d_multiple_input_channels(X, K[i])\n","        else:\n","            output = np.dstack((output, corr2d_multiple_input_channels(X, K[i])))\n","    \n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":452,"status":"ok","timestamp":1685979428042,"user":{"displayName":"Sapna Dhirawat","userId":"10829815467237624795"},"user_tz":-330},"id":"8KvPQ4eYLxGi","outputId":"3b390d54-8925-46cb-dec4-d5149f75656b"},"outputs":[{"name":"stdout","output_type":"stream","text":["my_K.shape = (4, 3, 2, 2)\n","[[[119. 155. 191. 227.]\n","  [149. 197. 245. 293.]]\n","\n"," [[209. 281. 353. 425.]\n","  [239. 323. 407. 491.]]]\n"]}],"source":["my_K = np.stack([new_K, new_K+1, new_K+2, new_K + 3], axis=0) \n","print(f\"my_K.shape = {my_K.shape}\")\n","\n","print(corr2d_multi_in_out(new_X, my_K)) # cross check the calculation"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uGjUjrXQLxGj"},"source":["### Q2: What is the computational and statistical benefits of stride larger than 1?? (not more than 20 word answer for each)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b41JYUvNFaRG"},"source":["\n","\n","##Computational benefits\n","  Reduced computational cost by performing fewer operations.\n","\n","  Faster processing and inference time.\n","\n","##Statistical benefits\n","\n","  Increased receptive field, capturing more context.\n","  \n","  Improved generalization by down-sampling and reducing spatial dimensions"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6_dSJFchLxGj"},"source":["### Q3: Now let's implement a model with just a single convolution layer, given X(input), Y(output) and K(kernel). Y is the output of the cross-correlation operation of K on X. You need to build a model to learn that kernel K.(try to print the kernel at each epoch)\n","\n","- hint : conv_layer(output_channels = 1, input_channels = 1, kerenl_size=(1, 2), bias=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":579,"status":"ok","timestamp":1685978516452,"user":{"displayName":"Sapna Dhirawat","userId":"10829815467237624795"},"user_tz":-330},"id":"uWAF0RPhLxGj","outputId":"96f0bf03-bb5e-4b19-a923-d2b4a0cf53b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1. 1. 0. 0. 0. 0. 1. 1.]\n"," [1. 1. 0. 0. 0. 0. 1. 1.]\n"," [1. 1. 0. 0. 0. 0. 1. 1.]\n"," [1. 1. 0. 0. 0. 0. 1. 1.]\n"," [1. 1. 0. 0. 0. 0. 1. 1.]\n"," [1. 1. 0. 0. 0. 0. 1. 1.]]\n"]}],"source":["X = np.ones((6, 8), dtype=np.float32)\n","X[:, 2:6] = 0\n","print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":492,"status":"ok","timestamp":1685986478461,"user":{"displayName":"Sapna Dhirawat","userId":"10829815467237624795"},"user_tz":-330},"id":"pr0_V5lpLxGj","outputId":"1ea468c2-f165-4ec4-b398-6ea7591a061e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-1. -1.]\n"," [-1. -1.]\n"," [-1. -1.]]\n"]}],"source":["K = np.array([1.0, -1.0], dtype=np.float32) # kernel, you need to learn this using a model\n","Y = corr2d(X, K) \n","print(Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"elapsed":639,"status":"error","timestamp":1685986422173,"user":{"displayName":"Sapna Dhirawat","userId":"10829815467237624795"},"user_tz":-330},"id":"4HSVk5foLxGk","outputId":"5cd5ea54-aed5-4d9b-f1af-bdaa255ed4ee"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-0f10a6c4c8ac>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this will be the input to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this is the output, to be used while calculation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-2\u001b[0m \u001b[0;31m# use thisÂ learningÂ rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 9 into shape (1,1,6,8)"]}],"source":["X = X.reshape((1, 1, 6, 8)) # this will be the input to the model\n","Y = Y.reshape((1, 1, 6, 7)) # this is the output, to be used while calculation loss\n","lr = 3e-2 # use thisÂ learningÂ rate"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-sWbsvXQLxGk"},"source":["### Q4: Complete the following function max_pool2d(X, K) which performs maxpooling with kernel size K on X and returns and two dim numpy array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Qjsv_v_LxGk"},"outputs":[],"source":["import numpy as np\n","def max_pool2d(X: np.array, K: tuple) -> np.array:\n","  k_h, k_w = K # (k_h = k_height and k_w= k_width)\n","  input_h, input_w = X.shape\n","  output_h = input_h // k_h\n","  output_w = input_w // k_w\n","\n","  output = np.zeros((output_h,output_w))\n","  \n","  for i in range(output_h):\n","    for j in range(output_h):\n","        pool_region = X[i*k_h : (i+1)*k_h, j*k_w : (j+1)*k_w]\n","        max_value = np.max(pool_region)\n","        output[i,j] = max_value\n","\n","  return output\n","\n","\n","    # write a function for this job"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1685979579601,"user":{"displayName":"Sapna Dhirawat","userId":"10829815467237624795"},"user_tz":-330},"id":"ZiLo_mjSLxGl","outputId":"f57814fa-afbe-4a1d-e21b-5eb1ee332257"},"outputs":[{"data":{"text/plain":["array([[4.]])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["X = np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]], dtype=np.float32)\n","max_pool2d(X, (2, 2)) # check if the output matches with your calculation"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"v-niSYY3LxGl"},"source":["# **SECTION:B**\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pfa0otIGY4vf"},"source":["### **JUST RUN THE CELLS AND VISUALIZE**( Nothing to code ðŸ™‚ )\n","\n","> Indented block\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mI1blAXSLxGl"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjSRQ2QuNf69"},"outputs":[],"source":["#importing the dataset\n","(X_train,Y_train),(X_test,Y_test) = tf.keras.datasets.cifar10.load_data()\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,Y_train,Y_test=train_test_split(X_train,Y_train,test_size=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhl64KENQSuq"},"outputs":[],"source":["# Print shape of all 4 variables: X_train,Y_train,X_test, and Y_test\n","print(X_train.shape)\n","print(X_test.shape)\n","print(Y_train.shape)\n","print(Y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oygw7B1fNm0O"},"outputs":[],"source":["#Lets visualize the CIFAR-10 dataset\n","\n","import random\n","figure = plt.figure(figsize=(6,6))\n","\n","for i in range(9):\n","  index = random.randint(0,len(X_train)-1) # showing the index_th image\n","  \n","  plt.subplot(3,3,i+1)\n","  plt.imshow(X_train[index])\n","  plt.title(Y_train[index])\n","  plt.axis(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIY0GmLjSMvK"},"outputs":[],"source":["\"\"\"\n","So you can probably notice here that the images are 3D(coloured) but still \n","not of great quality ( what can you expect from 32x32 image). Also there are\n","certain other factors which makes the classification a bit tougher than the \n","cases of 2D( the digit and the fashion data) you dealt before. We will try to\n","understand the difficulties and find probable solution for them.\n","\"\"\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ATuu2w_SNuCU"},"source":["![image.png](data:image/png;base64,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)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1yCGBaMSz5Q"},"outputs":[],"source":["\"\"\"\n","Lets first start with the CNN model discussed in the class for digit\n"," classification. Notice that I have changed the input shape for this usecase.\n"," Earlier it was (28,28,1) for the digit dataset.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNr0XFiKNpnE"},"outputs":[],"source":["# model initialization\n","cnn_model = tf.keras.Sequential()\n","\n","# adding the 1st layer of CNN\n","cnn_model.add(tf.keras.layers.Conv2D(26, (5,5), activation = 'relu', input_shape=(32,32,3)))\n","\n","# adding a maxpooling\n","cnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\n","\n","#adding another CNN layer\n","cnn_model.add(tf.keras.layers.Conv2D(16, (5,5), activation = 'relu'))\n","\n","# adding another maxpooling layer\n","cnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\n","\n","#flattening the layer\n","cnn_model.add(tf.keras.layers.Flatten())\n","\n","# 20 x 20 x 16\n","#dense layer\n","cnn_model.add(tf.keras.layers.Dense(64, activation='relu'))\n","\n","# final layer \n","cnn_model.add(tf.keras.layers.Dense(10, activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWoQIGv7N0tY"},"outputs":[],"source":["cnn_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n","                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                  metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yau4nndbTU4r"},"outputs":[],"source":["\"\"\"\n","Explain what is Adam optimizer below in atleast 250 words.[ read on web and explain ]\n","\n","Answer:Adam is a popular optimization algorithm used in deep learning. \n","It combines the good parts of 2 other algorithms, AdaGrad and RMSProp, to train neural networks in a good way. \n","Adam is especially useful when dealing with complex networks and messy data.\n","\n","The main idea behind Adam is to adjust the learning rate for each parameter in the network based on the gradients. \n","It does this by estimating the avg of the gradients and their squares. \n","After this Adam helps the network learn faster and makes it less sensitive to the settings chosen for training.\n","\n","Adam has three important components: momentum, AdaGrad, and RMSProp. \n","Momentum keeps track of the previous gradients that helps the network converge faster. \n","AdaGrad adjusts the learning rate based on the gradient magnitudes. \n","RMSProp normalizes the learning rate to prevent it from getting too small or too large during training.\n","\n","Combining  components Adam finds a good balance betwn adaptiveness and convergence speed. \n","It calculates the learning rate for each parameter by the gradients and their averages. \n","It also corrects for any biases in the initialization of the network.\n","\n","Adam is widely used because it is reliable,, efficient and easy to use. \n","It works well for diffrent deep learning tasks and is mostly the common choice for training neural networks. \n","Its performance can vary depending on the specific problem and dataset.\n","\"\"\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"x5rLNjUlPKrH"},"source":["### **Make sure that you are connected to GPU runtime other wise the training in next cell is going to take a long time**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RPCnjevN3W7"},"outputs":[],"source":["history = cnn_model.fit(X_train, Y_train, epochs=10, validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XMqr0nejTrrM"},"outputs":[],"source":["\"\"\" \n","Write about validation accuracy in 100 words.\n","\n","Answer: Validation accuracy is a count of how good a machine learning model performs on new, \n","unseen data. It gives us how much the model can tell the right labels  for this new data. \n","The validation dataset is different from the one used for training the model and \n","helps us understand how well the model can handle real-world situations. \n","\n","When the validation accuracy is high, it means the model is doing a good job. \n","But if the accuracy is low, it suggests that the model may not be performing well on unseen data or\n"," it may be focusing too much on the training data without generalizing well.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AkZcRmbN6SF"},"outputs":[],"source":["plt.plot(history.history['accuracy'],label=\"Train accuracy\")\n","plt.plot(history.history['val_accuracy'], label = \"Validation accuracy\")\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEQ0UpBDTN_9"},"outputs":[],"source":["\"\"\"\n","What do you think is happening? Is the model training or not?\n"," You can see that both the training and validation accuracy are\n"," just roaming around 0.1. \n"," One reason for this can be our model architecture. We had 26 filters\n"," in our first layer and 16 filters in our next layer. This funnel down approach\n"," works for dense layers but for Conv layers( which are good at feature extraction)\n"," we want them to extract more and more features.\n","\n"," So lets change that to funne up --> 16 and 32 in the layers respectively\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6HcpdtBuN8_F"},"outputs":[],"source":["# model initialization\n","cnn_model = tf.keras.Sequential()\n","\n","\"\"\"adding the 1st layer of CNN(Changed)\"\"\"\n","cnn_model.add(tf.keras.layers.Conv2D(16, (5,5), activation = 'relu', input_shape=(32,32,3)))\n","\n","# adding a maxpooling\n","cnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\n","\n","\"\"\"adding the 2nd layer of CNN(Changed)\"\"\"\n","cnn_model.add(tf.keras.layers.Conv2D(32, (5,5), activation = 'relu'))\n","\n","# adding another maxpooling layer\n","cnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\n","\n","#flattening the layer\n","cnn_model.add(tf.keras.layers.Flatten())\n","\n","# 20 x 20 x 16\n","#dense layer\n","cnn_model.add(tf.keras.layers.Dense(64, activation='relu'))\n","\n","# final layer \n","cnn_model.add(tf.keras.layers.Dense(10, activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFXQBRGPRjK6"},"outputs":[],"source":["cnn_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n","                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                  metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzC0iaowRmpb"},"outputs":[],"source":["history = cnn_model.fit(X_train, Y_train, epochs=20, validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdnpcDQoRrGo"},"outputs":[],"source":["plt.plot(history.history['accuracy'],label=\"Train accuracy\")\n","plt.plot(history.history['val_accuracy'], label = \"Validation accuracy\")\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MX4njN_DUwlU"},"outputs":[],"source":["\"\"\"\n","Did our model improved??\n","Are we done with the job?\n","What is happening after 5( roughly ) epochs? Why is there a gap between\n","training and validation accuracy?\n","\n","Our training accuracy reached to 0.73( and still increasing ) but \n","the validation accuracy seems to stagnate at 0.52.\n","\n","Is our model overfitting on the training data so much that it can't work well \n","on unseen data.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3l82eHENWI-R"},"outputs":[],"source":["\"\"\"\n","How are we gonna tackel this problem?\n","Well, we will see that in next part of this assignment.\n","Till then lets learn about about overfitting.\n","\"\"\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"m5PHy4o-XSpQ"},"source":["> **Well using Chatgpt to answer these question is something which everyone can think of right?? To create a difference write answer in your own terms after reading from web or reading the answer of Chatgpt.**\n","### **After all this is your midterm evaluation. Cheating is something we can catch easily** ( we have also done this )"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mFJ-Or0zsHUv"},"source":["# please take note of this change: \n","**[For all the subjective questions after this question( in the ss ) you need put the link of articles that you referred while searching about the question.]**\\\n","Ideally you should refer to at least 2 articles for each of those questions.\\\n","Also you do not need to go into deep maths of those( a qualitative answer is what we will be looking for)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7AzClJq2Z8zI"},"outputs":[],"source":["\"\"\"\n","[CLARIFICATION]: Reading and understanding by searching on Chatgpt is not \n","considered as cheating as long as you are writing that in your own word\n","( only problem is the crediblity of its information)\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNTS7DqAWXVl"},"outputs":[],"source":["\"\"\"\n","Question: What is overfitting and underfitting below ( atleast 200 words )\n","\n","\n","\n","\n","Answer: Overfitting and underfitting are two common problems in machine learning that can affect how well a model works.\n","\n","Overfitting happens when a model learns too much from the training data and becomes too more for it. \n","Its like the model tries too hard to remember every little detail in the training data the noise and irrelevant stuff etc. \n","This can make the model perform really well on the training data but it might not work well on new  unseen data. \n","\n","Underfitting happens when a model is too simple and dont catch the important patterns in the training data. \n","Its like having a model that dont learn more from the data and cannt make exact predictions. \n","Its like trying to solve a complex problem with a very basic approach.\n","\n","Both overfitting and underfitting are not good. We want our models to find a balance. \n","We want them to learn the important patterns from the data without getting too obsessed with every little detail or like being too simplistic.\n","\n","To prevent the overfitting we can use techniques like regularization. Regularization puts some rules on the model to keep it from becoming too complexx. \n","It helps the model focus on the important patterns and ignore the noise.\n","\n","To tackle underfitting, we can use more advanced models or add more features to capture the complexities of the data.\n","\n","Finding the right balance between model complexity and the available data is important. \n","Techniques like cross-validation can help us understand how well our models will work on new data.\n","\n","In simple terms overfitting is like studying just the questions from a past exam without understanding the concepts. \n","Underfitting is like having a model that is too simple to solve a complex problem. \n","We want our models to find the right balance and learn the important patterns from the data without getting too caught up in the details or being too simplistic.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfDCiJG1X652"},"outputs":[],"source":["\"\"\"\n","Question: What are regularization techniques in machine learning?(200 words)\n","\n","\n","\n","Answer: Regularization techniques in machine learning are method used to prevent  overfitting. \n","\n","Regularization techniques help in avoiding overfitting by controlling the complexity of the model. \n","They do this by adding some rules or penalties during the learning process.\n","\n","One technique is L1 regularization (or Lasso regularization) which adds a penalty to the model's loss function. \n","This penalty encourages the model to use fewer features and focus on the most important ones.\n","\n","Another technique is L2 regularization (or Ridge regularization) which also adds a penalty to the loss function. \n","This penalty makes the model's parameters smaller and reduces the impact of individual features.\n","\n","There are other techniques like dropout which randomly turns off some parts of the model during training to avoid relying too much on specific parts. \n","also another technique is early stopping, which stops the training process when the model starts to perform worse on a validation set.\n","\n","These regularization techniques help in controlling the complexity of the model and make it better at working with new data. \n","By preventing overfitting, they help the model generalize well and perform better on unseen examples.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VGMmK3XYEcn"},"outputs":[],"source":["\"\"\"\n","Question: What dropout layer and what does it do?( read it from Tensorflow.org and write in 200 words)\n","\n","Answer: A dropout layer is a special technique used in neural networks to make them better at learning from data. \n","It helps prevent  overfitting where the network becomes too focused on the training examples and \n","doesn't generalize well to new unseen data.\n","\n"," how dropout works\n","Imagine you have a bunch of neurons in a layer of your network. During training, \n","the dropout layer randomly turns off (or \"drops out\") some of these neurons which means they don't contribute to the computation. \n","The specific neurons that are dropped out change randomly with each training example.\n","\n","By dropping out neurons the network is forced to learn more independently. \n","It cannt rely too much on any single neuron bcoz it might be randomly dropped out at any time. \n","This encourages the network to learn more robust and diverse features from the data.\n","\n","The dropout rate determines how many neurons are dropped out. \n","For eg. if the dropout rate is 0.2, \n","then 20% of the neurons will be randomly dropped out during training. The dropout rate is commonly set between 20% to 50% depending on the problem.\n","\n","During testing or when making predictions dropout is turned off. But to make sure the network still works correctly, \n","the output of the dropout layer is scaled down by the dropout rate. This scaling ensures that the expected sum of the outputs remains the same.\n","\n","In simple terms dropout is like randomly temporarily removing some brain cells from the network during training to make it more resilient and adaptable. \n","This helps the network learn better and prevents it from getting too fixated on specific examples. \n","Its like giving the network a bit of randomness which can be helpful for improving its performance.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xxRFxeCzYqir"},"outputs":[],"source":["\"\"\"\n","Question: What is L1 normalization? write its formulae as well( atleast 200 words )\n","\n","Answer: L1 normalization is a technique used in machine learning or basically we can say  in Generative Adversarial Networks (GANs)\n"," to prevent overfitting and improve the performance of the model.\n","\n","In GANs, L1 normalization is commonly applied to the generator network. \n","The main idea behind L1 normalization is to encourage the generator to produce more focused and diverse outputs by penalizing large weights. \n","It does this by limiting the magnitude of the weights in the network.\n","\n","The formula for L1 normalization is as follows:\n","\n","L1_norm = Î» * E|w|\n","\n","In this formula, L1_norm represents the L1 normalization term Î» is a parameter that controls the strength of the regularization, \n","E denotes the sum of the absolute values of all the weights (w) in the generator network.\n","\n","By including the L1 normalization term in the generators loss function, \n","the generator is motivated to minimize the sum of the absolute values of its weights. As a result\n","the generator learns to rely on a smaller set of important weights leading to sparser and more diverse feature generation.\n","\n","The regularization parameter (Î») determines how much emphasis is given to L1 normalization in the overall loss function. \n","A higher value of Î» increases the penalty for large weights which encourages more sparsity in the generated outputs. \n","if Î» is set too high, the generator may produce overly simplistic outputs causing underfitting.\n","\n","summery - L1 normalization in GANs helps control the complexity of the generator network \n","improves the model's ability to generalize and encourages the generation of more varied and less correlated features.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qdqqsw9iYyMW"},"outputs":[],"source":["\"\"\"\n","Question: What is L2 normalization? write its formulae as well( atleast 200 words )\n","\n","Answer:L2 normalization, also known as Euclidean normalization, \n","is a technique used in machine learning and data analysis to make data vectors have the same length. \n","Its like resizing all the vectors so that they become equal in size. \n","This method is widely used in different fields such as image processing, natural language processing,& recommender systems.\n","\n","The formula for L2 normalization is quite simple. You take the original vector and divide it by the L2 norm of the vector. \n","The L2 norm is calculated by finding the square root of the sum of the squared values of all the components in the vector.\n","\n","By dividing each component of the original vector by its L2 norm, the L2 normalization scales the vector so that its length becomes 1. \n","Its like stretching or shrinking the vector but maintaining its direction. \n","Think of it as projecting the vector onto a circle or sphere with a radius of 1.\n","\n","L2 normalization is commonly used in machine learning to preprocess data before feeding it into algorithms. \n","One important use is to ensure that all features or data points have equal importance or influence in computations. \n","This is especially useful when dealing with data that has different scales or variations. \n","By normalizing the data, you can prevent certain features from dominating the learning process just because they have larger values.\n","\n","Another benefit of L2 normalization is that it can be used for similarity calculations. \n","Since it emphasizes the direction of vectors rather than their magnitudes it becomes easier to compare vectors based on their orientations.\n","\n","Overall- L2 normalization is a handy technique to standardize the length and direction of data vectors, \n","making them more comparable and suitable for various machine learning tasks.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-U7uWKxY0yv"},"outputs":[],"source":["\"\"\"\n","Question: What is data augmentation techniques and why is it needed in machine learning?( atleast 200 words )\n","\n","Answer: Data augmentation techniques are used in machine learning to make the training dataset bigger and more diverse. \n","This is done by applying different changes or modifications to the existing data, \n","creating new samples that are similar but not exactly the same as the original data. \n","these new samples are then added to the training dataset giving the model more varied examples to learn from.\n","\n","Data augmentation is important for several reasons. \n","firstly - it can be difficult and expensive to collect a large labeled dataset for training. \n","data augmentation helps solve this problem by generating more training samples from the existing ones, \n","effectively making the dataset larger and improving the model's ability to learn.\n","\n","secondly - data augmentation introduces variations and diversity into the training data, \n","which makes the model better at handling different situations and reduces overfitting. \n","By exposing the model to augmented samples it learns to handle transformations, noise, \n","or distortions that can happen in real-world scenarios. \n","This makes the model more adaptable to unseen data and improves its overall performance.\n","\n","lastly - data augmentation is useful for dealing with class imbalance. \n","In many classification problems some classes have fewer examples than others, \n","which can lead to biased models. By applying augmentation techniques to the minority class samples, \n","the class distribution can be balanced, preventing the model from favoring the majority class and \n","improving its ability to classify all classes accurately.\n","\n","Common data augmentation techniques include changing images by rotating, scaling,\n"," flipping, cropping, or adding noise. In natural language processing techniques like replacing words,\n","3inserting or deleting words can be used. These techniques increase the diversity of the training data and \n","help the model handle different variations and uncertainties in real-world situations.\n","\n","summary-  data augmentation is important in machine learning \n","because it helps overcome the limitations of limited training data \n","improves the model's ability to handle different situations reduces overfitting, \n","and addresses class imbalance. By creating more diverse training samples \n","the model can learn from a wider range of examples, leading to better performance and accuracy.\n","\n","SOURCE - ALL from chat gpt\n","\"\"\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Om6SQ7EBu0G5"},"source":["####**Did you take note of the change in the cell at the start of these \n","\n","---\n","\n","subjective questions ?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5CRLaL2UvKNW"},"outputs":[],"source":["\"ANS(Yes/No): no  \""]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1KCVDGjCVRps4cxNveYzmqn5j-LV4NFx6","timestamp":1685986485678},{"file_id":"1iNF8kFIiTwdeB5vw_8S_Q0uX6eX_elDt","timestamp":1685809572595}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
